# Enhancing Underwater Image Quality: A Dynamic Graph and Low-Rank Attention Approach
## About
To tackle the combined challenges of color distortion, nonuniform degradation, and multiscale blurring in underwater images, we propose the collaborative DGMLA-Net framework, which integrates the spatial dependency modeling strengths of GNNs with the multiscale and efficiency advantages of latent attention to improve overall enhancement quality.
## Experimental Results
The comparative results of our model against existing state-of-the-art methods on the UIEB dataset are presented in the figure below. It can be observed that images processed by our model exhibit superior visual quality compared to those generated by other methods.
![experimental results](https://github.com/dududu683/gashsjjddh/blob/main/UIEB.png)
Qualitative comparisons on UIEB:(a)Raw (b)Reference (c)UDCP (d)Fusion (e)WWPF (f)HFM (g)U-shape (h)UDAformer (i)HCLR-Net (j)GC-UIE (k)FUGN (l)UDnet (m)Ours
## Requirements
Before executing the code, make sure all required dependencies listed herein are installed.
```
torch==2.2.1+cu121  
torchvision==0.17.1+cu121  
torch_geometric==2.5.0  
torch_scatter==2.1.2+pt22cu121  
torch_sparse==0.6.18+pt22cu121  
numpy==1.26.4  
Pillow==10.3.0  
tqdm==4.66.4  
warmup_scheduler==0.3  
scikit-image==0.23.2  
torchinfo==1.8.0  
opencv-python==4.9.0.80
```
You can install all dependencies at once via `pip install -r requirements.txt`.

##  Datasets
All datasets utilized in this study are publicly available, and the links to access them are provided below:
UIEB : https://li-chongyi.github.io/proj_benchmark.html
EUVP : http://irvlab.cs.umn.edu/resources/euvp-dataset
UFO-120 :  http://irvlab.cs.umn.edu/resources/ufo-120-dataset
U45 : https://github.com/IPNUISTlegal/underwater-test-dataset-U45-
## Train
- Before training the model, you need to download the dataset and place it in  `dataset/train`.
- Start training with the following command: `python train.py`
- Then, locate the trained model in  `checkpoint/` .
## Test
- Before testing, place the test dataset in `testdata/`.
- Run `python test.py`
- Find the results in `results/`.
## Citation
If you use this code in your research, please cite:  
```
@article{wang2025underwater, 
author = {Wang, Yan and Du, Xiaojuan and Luo, Wenjuan and Hou, Yufei}, 
title = {Enhancing Underwater Image Quality: A Dynamic Graph and Low-Rank Attention Approach}, 
journal = {The Visual Computer}, 
year = {2025} }
```
